{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1a3348",
   "metadata": {},
   "source": [
    "# ðŸ§  VM Placement Using MORL and Heuristic Algorithms\n",
    "\n",
    "This Python script simulates the **Virtual Machine (VM) placement problem** using:\n",
    "\n",
    "- ðŸ§© **Heuristic Algorithms**: Best Fit, Worst Fit\n",
    "- ðŸŽ¯ **Multi-Objective Reinforcement Learning (MORL)**: Optimized via Chebyshev scalarization and Q-learning\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "Minimize:\n",
    "- âš¡ **Energy Consumption** â€” Based on PM power profiles\n",
    "- ðŸ§® **Resource Wastage** â€” Difference between allocated and available CPU/memory\n",
    "\n",
    "## ðŸ§± Modules Overview\n",
    "- **`PM`**: Physical Machine with CPU, memory, and power usage\n",
    "- **`VM`**: Virtual Machine with CPU and memory demands\n",
    "- **`VMPEnvironment`**: Simulates VM-to-PM placement dynamics\n",
    "- **`VMPMORLAgent`**: Learns placement using Chebyshev scalarization\n",
    "- **`train_vmpmorl`**: Trains the MORL agent using Q-learning\n",
    "- **`best_fit`, `worst_fit`**: Greedy heuristics for placement\n",
    "- **`simulate_algorithm`**: Executes and evaluates strategies\n",
    "\n",
    "## ðŸš€ What This Script Does\n",
    "1. Trains a MORL agent on 10 VMs and 10 PMs\n",
    "2. Applies Best Fit and Worst Fit heuristics\n",
    "3. Compares energy & resource wastage via bar plots\n",
    "\n",
    "## ðŸ“Š Output\n",
    "Visual comparison of strategies:\n",
    "- Energy efficiency\n",
    "- Resource utilization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03285f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class PM:\n",
    "    \"\"\"Physical Machine (PM) with CPU and memory resources.\"\"\"\n",
    "    def __init__(self, cpu_cap, mem_cap, idle_power=120, full_power=185):\n",
    "        self.cpu_cap = cpu_cap\n",
    "        self.mem_cap = mem_cap\n",
    "        self.cpu_used = 0.0\n",
    "        self.mem_used = 0.0\n",
    "        self.idle_power = idle_power\n",
    "        self.full_power = full_power\n",
    "        self.vm_list = []  # Track VMs assigned to this PM\n",
    "\n",
    "    def utilization(self):\n",
    "        \"\"\"Return normalized CPU and memory utilization.\"\"\"\n",
    "        return (self.cpu_used / self.cpu_cap, self.mem_used / self.mem_cap)\n",
    "\n",
    "    def add_vm(self, vm_id, vm_cpu, vm_mem):\n",
    "        \"\"\"Attempt to place a VM on this PM. Returns True if successful.\"\"\"\n",
    "        new_cpu = self.cpu_used + vm_cpu\n",
    "        new_mem = self.mem_used + vm_mem\n",
    "        if new_cpu <= self.cpu_cap and new_mem <= self.mem_cap:\n",
    "            self.cpu_used = new_cpu\n",
    "            self.mem_used = new_mem\n",
    "            self.vm_list.append((vm_id, vm_cpu, vm_mem))  # Store VM ID and resource usage\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class VM:\n",
    "    \"\"\"Virtual Machine (VM) with CPU and memory demands.\"\"\"\n",
    "    def __init__(self, vm_id, cpu_demand, mem_demand):\n",
    "        self.id = vm_id\n",
    "        self.cpu = cpu_demand\n",
    "        self.mem = mem_demand\n",
    "\n",
    "class VMPEnvironment:\n",
    "    \"\"\"Environment for VM placement.\"\"\"\n",
    "    def __init__(self, pm_list, vm_list, threshold=0.9):\n",
    "        self.pms = pm_list\n",
    "        self.vms = vm_list\n",
    "        self.threshold = threshold\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for a new episode.\"\"\"\n",
    "        self.current_step = 0\n",
    "        for pm in self.pms:\n",
    "            pm.cpu_used = 0.0\n",
    "            pm.mem_used = 0.0\n",
    "            pm.vm_list = []  # Clear VM list\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Return state as normalized CPU and memory utilization for all PMs.\"\"\"\n",
    "        state = []\n",
    "        for pm in self.pms:\n",
    "            cpu_util, mem_util = pm.utilization()\n",
    "            state.extend([cpu_util, mem_util])\n",
    "        return np.array(state)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Place VM on selected PM. Return next state, rewards, done.\"\"\"\n",
    "        vm = self.vms[self.current_step]\n",
    "        pm = self.pms[action]\n",
    "\n",
    "        # Check if PM can accommodate VM\n",
    "        success = pm.add_vm(vm.id, vm.cpu, vm.mem)\n",
    "        if not success:\n",
    "            raise ValueError(\"Invalid action: PM cannot accommodate VM.\")\n",
    "\n",
    "        # Calculate rewards\n",
    "        energy_reward = self._energy_reward(pm)\n",
    "        wastage_reward = self._wastage_reward(pm)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.vms))\n",
    "        next_state = self._get_state()\n",
    "\n",
    "        return next_state, [energy_reward, wastage_reward], done, action\n",
    "\n",
    "    def _energy_reward(self, pm):\n",
    "        \"\"\"Reward based on energy consumption.\"\"\"\n",
    "        if pm.cpu_used == 0 and pm.mem_used == 0:\n",
    "            return 0\n",
    "        utilization = pm.cpu_used / pm.cpu_cap\n",
    "        energy = (pm.full_power - pm.idle_power) * utilization + pm.idle_power\n",
    "        return -energy  # Negative because we want to minimize\n",
    "\n",
    "    def _wastage_reward(self, pm):\n",
    "        \"\"\"Reward based on resource wastage.\"\"\"\n",
    "        residual_cpu = self.threshold - (pm.cpu_used / pm.cpu_cap)\n",
    "        residual_mem = self.threshold - (pm.mem_used / pm.mem_cap)\n",
    "        numerator = abs(residual_cpu - residual_mem)\n",
    "        denominator = (pm.cpu_used / pm.cpu_cap) + (pm.mem_used / pm.mem_cap) + 0.0001\n",
    "        wastage = numerator / denominator\n",
    "        return -wastage  # Negative to minimize\n",
    "\n",
    "class VMPMORLAgent:\n",
    "    \"\"\"Multi-objective RL agent with Chebyshev scalarization.\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=0.1, gamma=0.8, epsilon=0.1):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.weights = [0.5, 0.5]  # Equal weight for energy and wastage\n",
    "        self.z_star = [0, 0]  # Utopian point (initialized to 0)\n",
    "\n",
    "        # Q-tables for each objective\n",
    "        self.q_energy = defaultdict(lambda: np.zeros(action_size))\n",
    "        self.q_wastage = defaultdict(lambda: np.zeros(action_size))\n",
    "\n",
    "    def chebyshev_scalarization(self, rewards):\n",
    "        \"\"\"Compute Chebyshev scalarized value.\"\"\"\n",
    "        term1 = self.weights[0] * abs(rewards[0] - self.z_star[0])\n",
    "        term2 = self.weights[1] * abs(rewards[1] - self.z_star[1])\n",
    "        return max(term1, term2)\n",
    "\n",
    "def train_vmpmorl(pm_list, vm_list, episodes=500):\n",
    "    env = VMPEnvironment(pm_list, vm_list)\n",
    "    agent = VMPMORLAgent(state_size=2*len(pm_list), action_size=len(pm_list))\n",
    "    pareto_front = []\n",
    "    best_pm_vm_mapping = {}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards = [0, 0]\n",
    "\n",
    "        while not done:\n",
    "            available_actions = [i for i, pm in enumerate(env.pms)\n",
    "                                 if pm.cpu_used + env.vms[env.current_step].cpu <= pm.cpu_cap and\n",
    "                                 pm.mem_used + env.vms[env.current_step].mem <= pm.mem_cap]\n",
    "            action = random.choice(available_actions)\n",
    "            next_state, rewards, done, assigned_pm = env.step(action)\n",
    "\n",
    "            total_rewards[0] += rewards[0]\n",
    "            total_rewards[1] += rewards[1]\n",
    "\n",
    "        # Add to Pareto front if not dominated\n",
    "        is_dominated = any(sol[0] <= total_rewards[0] and sol[1] <= total_rewards[1] for sol in pareto_front)\n",
    "        if not is_dominated:\n",
    "            pareto_front.append(total_rewards)\n",
    "\n",
    "            # Store best VM to PM mapping\n",
    "            best_pm_vm_mapping = {i: pm.vm_list for i, pm in enumerate(env.pms) if pm.vm_list}\n",
    "\n",
    "    # Applying Chebyshev Scalarization to select the best solution\n",
    "    best_solution = min(pareto_front, key=agent.chebyshev_scalarization)\n",
    "\n",
    "    return agent, pareto_front, best_solution, best_pm_vm_mapping\n",
    "\n",
    "# Heuristic Algorithms: Best Fit, Worst Fit, First Fit\n",
    "def best_fit(pms, vm):\n",
    "    \"\"\"Best Fit: Place VM in PM with least remaining space after placement.\"\"\"\n",
    "    min_space = float('inf')\n",
    "    best_pm = -1\n",
    "    for i, pm in enumerate(pms):\n",
    "        if pm.cpu_used + vm.cpu <= pm.cpu_cap and pm.mem_used + vm.mem <= pm.mem_cap:\n",
    "            remaining_space = (pm.cpu_cap - (pm.cpu_used + vm.cpu)) + (pm.mem_cap - (pm.mem_used + vm.mem))\n",
    "            if remaining_space < min_space:\n",
    "                min_space = remaining_space\n",
    "                best_pm = i\n",
    "    return best_pm\n",
    "\n",
    "def worst_fit(pms, vm):\n",
    "    \"\"\"Worst Fit: Place VM in PM with most remaining space after placement.\"\"\"\n",
    "    max_space = -1\n",
    "    worst_pm = -1\n",
    "    for i, pm in enumerate(pms):\n",
    "        if pm.cpu_used + vm.cpu <= pm.cpu_cap and pm.mem_used + vm.mem <= pm.mem_cap:\n",
    "            remaining_space = (pm.cpu_cap - (pm.cpu_used + vm.cpu)) + (pm.mem_cap - (pm.mem_used + vm.mem))\n",
    "            if remaining_space > max_space:\n",
    "                max_space = remaining_space\n",
    "                worst_pm = i\n",
    "    return worst_pm\n",
    "\n",
    "def simulate_algorithm(pm_list, vm_list, algorithm_func):\n",
    "    \"\"\"Simulate VM placement with given heuristic algorithm.\"\"\"\n",
    "    for pm in pm_list:\n",
    "        pm.cpu_used = 0.0\n",
    "        pm.mem_used = 0.0\n",
    "        pm.vm_list = []\n",
    "\n",
    "    for vm in vm_list:\n",
    "        selected_pm_idx = algorithm_func(pm_list, vm)\n",
    "        if selected_pm_idx == -1:\n",
    "            raise ValueError(f\"VM {vm.id} could not be placed using {algorithm_func.__name__}.\")\n",
    "        pm_list[selected_pm_idx].add_vm(vm.id, vm.cpu, vm.mem)\n",
    "\n",
    "    total_energy = sum((pm.full_power - pm.idle_power) * (pm.cpu_used / pm.cpu_cap) + pm.idle_power\n",
    "                       for pm in pm_list if pm.cpu_used > 0)\n",
    "    total_wastage = sum(abs((pm.cpu_cap - pm.cpu_used) - (pm.mem_cap - pm.mem_used)) /\n",
    "                        ((pm.cpu_used / pm.cpu_cap) + (pm.mem_used / pm.mem_cap) + 0.0001)\n",
    "                        for pm in pm_list if pm.cpu_used > 0)\n",
    "\n",
    "    return total_energy, total_wastage\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example setup\n",
    "    pms = [PM(1.0, 1.0) for _ in range(10)]  # 10 PMs\n",
    "    vms = [VM(i, random.uniform(0.2, 0.4), random.uniform(0.2, 0.4)) for i in range(10)]  # 10 VMs\n",
    "    for vm in vms:\n",
    "        print(f\"VM {vm.id} -> CPU: {vm.cpu:.2f}, Memory: {vm.mem:.2f}\")\n",
    "\n",
    "    # Train MORL agent\n",
    "    agent, pareto_front, best_solution, best_pm_vm_mapping = train_vmpmorl(pms, vms, episodes=100)\n",
    "\n",
    "    # Simulate heuristic algorithms\n",
    "    bf_energy, bf_wastage = simulate_algorithm(pms, vms, best_fit)\n",
    "    wf_energy, wf_wastage = simulate_algorithm(pms, vms, worst_fit)\n",
    "\n",
    "    # MORL results\n",
    "    morl_energy = -best_solution[0]\n",
    "    morl_wastage = -best_solution[1]\n",
    "\n",
    "    # Plotting results\n",
    "    algorithms = ['Best Fit', 'Worst Fit', 'MORL']\n",
    "    energy_values = [bf_energy, wf_energy, morl_energy]\n",
    "    wastage_values = [bf_wastage, wf_wastage, morl_wastage]\n",
    "\n",
    "    # Bar Plot for Energy and Wastage\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Energy plot\n",
    "    ax[0].bar(algorithms, energy_values, color=['blue', 'orange', 'green'])\n",
    "    ax[0].set_title('Energy Consumption Comparison')\n",
    "    ax[0].set_ylabel('Energy (W)')\n",
    "    ax[0].set_xlabel('Algorithm')\n",
    "\n",
    "    # Wastage plot\n",
    "    ax[1].bar(algorithms, wastage_values, color=['blue', 'orange', 'green'])\n",
    "    ax[1].set_title('Resource Wastage Comparison')\n",
    "    ax[1].set_ylabel('Wastage (%)')\n",
    "    ax[1].set_xlabel('Algorithm')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
